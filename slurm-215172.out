make: `bin/statsNaive' is up to date.
comp
1000 particles
[lonsdale-n001:14879] *** Process received signal ***
[lonsdale-n001:14879] Signal: Segmentation fault (11)
[lonsdale-n001:14879] Signal code: Address not mapped (1)
[lonsdale-n001:14879] Failing at address: 0xfffffffc00e9e520
[lonsdale-n001:14879] [ 0] /lib64/libpthread.so.0() [0x396280f710]
[lonsdale-n001:14879] [ 1] bin/statsNaive(compareMultipleParticles+0x530) [0x403db3]
[lonsdale-n001:14879] [ 2] bin/statsNaive(main+0xcc1) [0x40539b]
[lonsdale-n001:14879] [ 3] /lib64/libc.so.6(__libc_start_main+0xfd) [0x396201ed5d]
[lonsdale-n001:14879] [ 4] bin/statsNaive() [0x401779]
[lonsdale-n001:14879] *** End of error message ***
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 14879 on node lonsdale-n001.cluster exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------

real	8m16.610s
user	8m16.453s
sys	0m0.056s
160 particles
[lonsdale-n001:15094] *** Process received signal ***
[lonsdale-n001:15094] Signal: Segmentation fault (11)
[lonsdale-n001:15094] Signal code: Address not mapped (1)
[lonsdale-n001:15094] Failing at address: 0xfffffffc00ce5c40
[lonsdale-n001:15094] [ 0] /lib64/libpthread.so.0() [0x396280f710]
[lonsdale-n001:15094] [ 1] bin/statsNaive(compareMultipleParticles+0x530) [0x403db3]
[lonsdale-n001:15094] [ 2] bin/statsNaive(main+0xcc1) [0x40539b]
[lonsdale-n001:15094] [ 3] /lib64/libc.so.6(__libc_start_main+0xfd) [0x396201ed5d]
[lonsdale-n001:15094] [ 4] bin/statsNaive() [0x401779]
[lonsdale-n001:15094] *** End of error message ***
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 15094 on node lonsdale-n001.cluster exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------

real	0m2.080s
user	0m2.030s
sys	0m0.038s
120 particles
[lonsdale-n001:15098] *** Process received signal ***
[lonsdale-n001:15098] Signal: Segmentation fault (11)
[lonsdale-n001:15098] Signal code: Address not mapped (1)
[lonsdale-n001:15098] Failing at address: 0xfffffffc00a15920
[lonsdale-n001:15098] [ 0] /lib64/libpthread.so.0() [0x396280f710]
[lonsdale-n001:15098] [ 1] bin/statsNaive(compareMultipleParticles+0x530) [0x403db3]
[lonsdale-n001:15098] [ 2] bin/statsNaive(main+0xcc1) [0x40539b]
[lonsdale-n001:15098] [ 3] /lib64/libc.so.6(__libc_start_main+0xfd) [0x396201ed5d]
[lonsdale-n001:15098] [ 4] bin/statsNaive() [0x401779]
[lonsdale-n001:15098] *** End of error message ***
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 15098 on node lonsdale-n001.cluster exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------

real	0m0.920s
user	0m0.870s
sys	0m0.039s
80 particles
could not open file: compStatsInit/comp10.initcommand line options failure
--------------------------------------------------------------------------
mpirun has exited due to process rank 0 with PID 15102 on
node lonsdale-n001.cluster exiting improperly. There are two reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).
--------------------------------------------------------------------------

real	0m0.076s
user	0m0.026s
sys	0m0.041s


###############################################################################
TCHPC Cluster: lonsdale
Job 215172 (histogram) for User 'rooneydt' in Account 'mschpc'
Finished at: Sun Aug 23 16:10:34 IST 2015

Job completion status:
======================

       JobID    JobName AllocCPUS NTasks NNodes     MaxRSS    MaxRSSNode  MaxDiskRead MaxDiskWrite    Elapsed      State ExitCode 
------------ ---------- --------- ------ ------ ---------- ------------- ------------ ------------ ---------- ---------- -------- 
215172        histogram         8             1                                                      00:08:21     FAILED      1:0 
215172.batch      batch         8      1      1     11752K lonsdale-n001        0.42M        0.00M   00:08:21     FAILED      1:0 


Job details:
============

JobId=215172 JobName=histogram
   UserId=rooneydt(5482) GroupId=rooneydt(9287)
   Priority=10065315 Nice=0 Account=mschpc QOS=normal
   JobState=COMPLETING Reason=NonZeroExitCode Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=1:0
   DerivedExitCode=0:0
   RunTime=00:08:21 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2015-08-23T16:02:13 EligibleTime=2015-08-23T16:02:13
   StartTime=2015-08-23T16:02:13 EndTime=2015-08-23T16:10:34
   PreemptTime=None SuspendTime=None SecsPreSuspend=0
   Partition=debug AllocNode:Sid=lonsdale01:30214
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=
   BatchHost=lonsdale-n001
   NumNodes=1 NumCPUs=8 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
     Nodes=lonsdale-n001 CPU_IDs=0-7 Mem=15000
   MinCPUsNode=1 MinMemoryNode=15000M MinTmpDiskNode=0
   Features=(null) Gres=(null) Reservation=(null)
   Shared=0 Contiguous=0 Licenses=(null) Network=(null)
   Command=/home/users/mschpc/2014/rooneydt/3body/hist2.sh
   WorkDir=/home/users/mschpc/2014/rooneydt/3body
   StdErr=/home/users/mschpc/2014/rooneydt/3body/slurm-215172.out
   StdIn=/dev/null
   StdOut=/home/users/mschpc/2014/rooneydt/3body/slurm-215172.out


Disk quota details:
===================

Quota Type           Name  Filesystem     Usage in MB     Limit in MB    % Used
-------------------------------------------------------------------------------
USER             rooneydt       /home              34          51,200     0.07%

GROUP              mschpc   /projects          23,731          51,200    46.35%


SLURM Bank Statement:
=====================

User           Usage |        Account     Usage | Account Limit Available (CPU hrs)
---------- --------- + -------------- --------- + ------------- ---------

aljohani          22 |         MSCHPC   106,052 |       400,000   293,948
boycek           249 |         MSCHPC   106,052 |       400,000   293,948
dalyso           244 |         MSCHPC   106,052 |       400,000   293,948
deleligt         709 |         MSCHPC   106,052 |       400,000   293,948
dowlindo         563 |         MSCHPC   106,052 |       400,000   293,948
dunneff        4,157 |         MSCHPC   106,052 |       400,000   293,948
elynch         2,903 |         MSCHPC   106,052 |       400,000   293,948
hannigs            6 |         MSCHPC   106,052 |       400,000   293,948
hernonma         457 |         MSCHPC   106,052 |       400,000   293,948
holtonmi           0 |         MSCHPC   106,052 |       400,000   293,948
howardrj          37 |         MSCHPC   106,052 |       400,000   293,948
hynesr         1,365 |         MSCHPC   106,052 |       400,000   293,948
jabehan        3,965 |         MSCHPC   106,052 |       400,000   293,948
jbulava          158 |         MSCHPC   106,052 |       400,000   293,948
jose               0 |         MSCHPC   106,052 |       400,000   293,948
kcleary           67 |         MSCHPC   106,052 |       400,000   293,948
lambem           514 |         MSCHPC   106,052 |       400,000   293,948
liul1            505 |         MSCHPC   106,052 |       400,000   293,948
luoq           2,782 |         MSCHPC   106,052 |       400,000   293,948
makirby        1,126 |         MSCHPC   106,052 |       400,000   293,948
mcbridne       2,142 |         MSCHPC   106,052 |       400,000   293,948
mehtav            82 |         MSCHPC   106,052 |       400,000   293,948
melbyrne         300 |         MSCHPC   106,052 |       400,000   293,948
murphd37      15,590 |         MSCHPC   106,052 |       400,000   293,948
murrayb8       2,421 |         MSCHPC   106,052 |       400,000   293,948
nobyrnes       1,513 |         MSCHPC   106,052 |       400,000   293,948
oconnm28         619 |         MSCHPC   106,052 |       400,000   293,948
ormondca       1,661 |         MSCHPC   106,052 |       400,000   293,948
osheac9          968 |         MSCHPC   106,052 |       400,000   293,948
osullm40       6,396 |         MSCHPC   106,052 |       400,000   293,948
paddy          1,319 |         MSCHPC   106,052 |       400,000   293,948
phalpin          592 |         MSCHPC   106,052 |       400,000   293,948
purdyd           173 |         MSCHPC   106,052 |       400,000   293,948
rooneydt         364 |         MSCHPC   106,052 |       400,000   293,948
sharding       1,196 |         MSCHPC   106,052 |       400,000   293,948
sikelleh         990 |         MSCHPC   106,052 |       400,000   293,948
simpsoao         509 |         MSCHPC   106,052 |       400,000   293,948
smurray4         514 |         MSCHPC   106,052 |       400,000   293,948
spellacl       1,730 |         MSCHPC   106,052 |       400,000   293,948
vetsaa            80 |         MSCHPC   106,052 |       400,000   293,948
wechen         7,593 |         MSCHPC   106,052 |       400,000   293,948
wyattg           200 |         MSCHPC   106,052 |       400,000   293,948
zhuang         8,654 |         MSCHPC   106,052 |       400,000   293,948


Acknowledgements:
=================

Note that usage of TCHPC Resources *must* be acknowledged in all publications.

Please see this page for details relevant to this cluster:

http://www.tchpc.tcd.ie/resources/acknowledgementpolicy

################################################################################
