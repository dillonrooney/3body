make: `bin/naive' is up to date.
compare
1000 particles
1 particle checking
rank 0 diff = 0

real	8m18.007s
user	8m17.576s
sys	0m0.169s


###############################################################################
TCHPC Cluster: lonsdale
Job 214214 (1024particles) for User 'rooneydt' in Account 'mschpc'
Finished at: Wed Aug 19 23:18:21 IST 2015

Job completion status:
======================

       JobID    JobName AllocCPUS NTasks NNodes     MaxRSS    MaxRSSNode  MaxDiskRead MaxDiskWrite    Elapsed      State ExitCode 
------------ ---------- --------- ------ ------ ---------- ------------- ------------ ------------ ---------- ---------- -------- 
214214       1024parti+         8             1                                                      00:08:20  COMPLETED      0:0 
214214.batch      batch         8      1      1     11752K lonsdale-n003        0.42M        0.00M   00:08:20  COMPLETED      0:0 


Job details:
============

JobId=214214 JobName=1024particles
   UserId=rooneydt(5482) GroupId=rooneydt(9287)
   Priority=10127010 Nice=0 Account=mschpc QOS=normal
   JobState=COMPLETING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   DerivedExitCode=0:0
   RunTime=00:08:20 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2015-08-19T23:10:00 EligibleTime=2015-08-19T23:10:00
   StartTime=2015-08-19T23:10:01 EndTime=2015-08-19T23:18:21
   PreemptTime=None SuspendTime=None SecsPreSuspend=0
   Partition=debug AllocNode:Sid=lonsdale01:19188
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=
   BatchHost=lonsdale-n003
   NumNodes=1 NumCPUs=8 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
     Nodes=lonsdale-n003 CPU_IDs=0-7 Mem=15000
   MinCPUsNode=1 MinMemoryNode=15000M MinTmpDiskNode=0
   Features=(null) Gres=(null) Reservation=(null)
   Shared=0 Contiguous=0 Licenses=(null) Network=(null)
   Command=/home/users/mschpc/2014/rooneydt/3body/sbatch4.sh
   WorkDir=/home/users/mschpc/2014/rooneydt/3body
   StdErr=/home/users/mschpc/2014/rooneydt/3body/slurm-214214.out
   StdIn=/dev/null
   StdOut=/home/users/mschpc/2014/rooneydt/3body/slurm-214214.out


Disk quota details:
===================

Quota Type           Name  Filesystem     Usage in MB     Limit in MB    % Used
-------------------------------------------------------------------------------
USER             rooneydt       /home              30          51,200     0.06%

GROUP              mschpc   /projects          23,731          51,200    46.35%


SLURM Bank Statement:
=====================

User           Usage |        Account     Usage | Account Limit Available (CPU hrs)
---------- --------- + -------------- --------- + ------------- ---------

aljohani          22 |         MSCHPC   103,205 |       400,000   296,795
boycek           430 |         MSCHPC   103,205 |       400,000   296,795
dalyso           200 |         MSCHPC   103,205 |       400,000   296,795
deleligt         709 |         MSCHPC   103,205 |       400,000   296,795
dowlindo         563 |         MSCHPC   103,205 |       400,000   296,795
dunneff        4,157 |         MSCHPC   103,205 |       400,000   296,795
elynch         2,903 |         MSCHPC   103,205 |       400,000   296,795
hannigs            6 |         MSCHPC   103,205 |       400,000   296,795
hernonma         457 |         MSCHPC   103,205 |       400,000   296,795
holtonmi           0 |         MSCHPC   103,205 |       400,000   296,795
howardrj          37 |         MSCHPC   103,205 |       400,000   296,795
hynesr         1,365 |         MSCHPC   103,205 |       400,000   296,795
jabehan        3,862 |         MSCHPC   103,205 |       400,000   296,795
jbulava          158 |         MSCHPC   103,205 |       400,000   296,795
jose               0 |         MSCHPC   103,205 |       400,000   296,795
kcleary           67 |         MSCHPC   103,205 |       400,000   296,795
lambem           514 |         MSCHPC   103,205 |       400,000   296,795
liul1            504 |         MSCHPC   103,205 |       400,000   296,795
luoq           2,782 |         MSCHPC   103,205 |       400,000   296,795
makirby        1,126 |         MSCHPC   103,205 |       400,000   296,795
mcbridne       2,142 |         MSCHPC   103,205 |       400,000   296,795
mehtav            82 |         MSCHPC   103,205 |       400,000   296,795
melbyrne         302 |         MSCHPC   103,205 |       400,000   296,795
murphd37      15,590 |         MSCHPC   103,205 |       400,000   296,795
murrayb8       2,421 |         MSCHPC   103,205 |       400,000   296,795
nobyrnes       1,513 |         MSCHPC   103,205 |       400,000   296,795
oconnm28         619 |         MSCHPC   103,205 |       400,000   296,795
ormondca       1,661 |         MSCHPC   103,205 |       400,000   296,795
osheac9          968 |         MSCHPC   103,205 |       400,000   296,795
osullm40       3,809 |         MSCHPC   103,205 |       400,000   296,795
paddy          1,363 |         MSCHPC   103,205 |       400,000   296,795
phalpin          592 |         MSCHPC   103,205 |       400,000   296,795
purdyd           173 |         MSCHPC   103,205 |       400,000   296,795
rooneydt         319 |         MSCHPC   103,205 |       400,000   296,795
sharding       1,196 |         MSCHPC   103,205 |       400,000   296,795
sikelleh         842 |         MSCHPC   103,205 |       400,000   296,795
simpsoao         509 |         MSCHPC   103,205 |       400,000   296,795
smurray4         514 |         MSCHPC   103,205 |       400,000   296,795
spellacl       1,730 |         MSCHPC   103,205 |       400,000   296,795
vetsaa            80 |         MSCHPC   103,205 |       400,000   296,795
wechen         7,593 |         MSCHPC   103,205 |       400,000   296,795
wyattg           200 |         MSCHPC   103,205 |       400,000   296,795
zhuang         8,265 |         MSCHPC   103,205 |       400,000   296,795


Acknowledgements:
=================

Note that usage of TCHPC Resources *must* be acknowledged in all publications.

Please see this page for details relevant to this cluster:

http://www.tchpc.tcd.ie/resources/acknowledgementpolicy

################################################################################
