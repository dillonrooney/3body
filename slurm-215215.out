make: `bin/statsNaive' is up to date.
comp
1000 particles
751.734902	0.000044
2255.204706	0.000000
3758.674510	0.000000
5262.144314	0.000000
6765.614118	0.000000
8269.083922	0.000000
9772.553726	0.000000
11276.023530	0.000000
12779.493334	0.000000
14282.963138	0.000000
15786.432941	0.000000
17289.902745	0.000000
18793.372549	0.000000
20296.842353	0.000000
21800.312157	0.000000
comparison of dv statistics:rank 0: mean=215.864226	stdev=601.973643
rank 0 diff = 22552

real	8m14.209s
user	8m14.077s
sys	0m0.056s
160 particles
136.715186	0.000240
410.145559	0.000002
683.575931	0.000000
957.006304	0.000001
1230.436676	0.000001
1503.867049	0.000000
1777.297421	0.000000
2050.727794	0.000000
2324.158166	0.000000
2597.588539	0.000000
2871.018911	0.000000
3144.449284	0.000000
3417.879656	0.000000
3691.310029	0.000000
3964.740401	0.000001
comparison of dv statistics:rank 0: mean=39.329982	stdev=201.339887
rank 0 diff = 4101.46

real	0m2.087s
user	0m2.031s
sys	0m0.038s
120 particles
87.130764	0.000376
261.392293	0.000003
435.653821	0.000001
609.915350	0.000001
784.176879	0.000000
958.438407	0.000000
1132.699936	0.000000
1306.961464	0.000000
1481.222993	0.000000
1655.484521	0.000000
1829.746050	0.000000
2004.007579	0.000000
2178.269107	0.000000
2352.530636	0.000000
2526.792164	0.000001
comparison of dv statistics:rank 0: mean=25.267232	stdev=144.089375
rank 0 diff = 2613.92

real	0m0.927s
user	0m0.872s
sys	0m0.047s
80 particles
could not open file: compStatsInit/comp10.initcommand line options failure
--------------------------------------------------------------------------
mpirun has exited due to process rank 0 with PID 27821 on
node lonsdale-n001.cluster exiting improperly. There are two reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).
--------------------------------------------------------------------------

real	0m0.082s
user	0m0.026s
sys	0m0.041s


###############################################################################
TCHPC Cluster: lonsdale
Job 215215 (histogram) for User 'rooneydt' in Account 'mschpc'
Finished at: Sun Aug 23 22:09:06 IST 2015

Job completion status:
======================

       JobID    JobName AllocCPUS NTasks NNodes     MaxRSS    MaxRSSNode  MaxDiskRead MaxDiskWrite    Elapsed      State ExitCode 
------------ ---------- --------- ------ ------ ---------- ------------- ------------ ------------ ---------- ---------- -------- 
215215        histogram         8             1                                                      00:08:18     FAILED      1:0 
215215.batch      batch         8      1      1     11720K lonsdale-n001        0.42M        0.00M   00:08:18     FAILED      1:0 


Job details:
============

JobId=215215 JobName=histogram
   UserId=rooneydt(5482) GroupId=rooneydt(9287)
   Priority=10063970 Nice=0 Account=mschpc QOS=normal
   JobState=COMPLETING Reason=NonZeroExitCode Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=1:0
   DerivedExitCode=0:0
   RunTime=00:08:18 TimeLimit=00:30:00 TimeMin=N/A
   SubmitTime=2015-08-23T22:00:47 EligibleTime=2015-08-23T22:00:47
   StartTime=2015-08-23T22:00:48 EndTime=2015-08-23T22:09:06
   PreemptTime=None SuspendTime=None SecsPreSuspend=0
   Partition=debug AllocNode:Sid=lonsdale01:8939
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=
   BatchHost=lonsdale-n001
   NumNodes=1 NumCPUs=8 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
     Nodes=lonsdale-n001 CPU_IDs=0-7 Mem=15000
   MinCPUsNode=1 MinMemoryNode=15000M MinTmpDiskNode=0
   Features=(null) Gres=(null) Reservation=(null)
   Shared=0 Contiguous=0 Licenses=(null) Network=(null)
   Command=/home/users/mschpc/2014/rooneydt/3body/hist2.sh
   WorkDir=/home/users/mschpc/2014/rooneydt/3body
   StdErr=/home/users/mschpc/2014/rooneydt/3body/slurm-215215.out
   StdIn=/dev/null
   StdOut=/home/users/mschpc/2014/rooneydt/3body/slurm-215215.out


Disk quota details:
===================

Quota Type           Name  Filesystem     Usage in MB     Limit in MB    % Used
-------------------------------------------------------------------------------
USER             rooneydt       /home              36          51,200     0.07%

GROUP              mschpc   /projects          23,731          51,200    46.35%


SLURM Bank Statement:
=====================

User           Usage |        Account     Usage | Account Limit Available (CPU hrs)
---------- --------- + -------------- --------- + ------------- ---------

aljohani          22 |         MSCHPC   106,307 |       400,000   293,693
boycek           249 |         MSCHPC   106,307 |       400,000   293,693
dalyso           251 |         MSCHPC   106,307 |       400,000   293,693
deleligt         709 |         MSCHPC   106,307 |       400,000   293,693
dowlindo         563 |         MSCHPC   106,307 |       400,000   293,693
dunneff        4,157 |         MSCHPC   106,307 |       400,000   293,693
elynch         2,903 |         MSCHPC   106,307 |       400,000   293,693
hannigs            6 |         MSCHPC   106,307 |       400,000   293,693
hernonma         457 |         MSCHPC   106,307 |       400,000   293,693
holtonmi           0 |         MSCHPC   106,307 |       400,000   293,693
howardrj          37 |         MSCHPC   106,307 |       400,000   293,693
hynesr         1,365 |         MSCHPC   106,307 |       400,000   293,693
jabehan        4,020 |         MSCHPC   106,307 |       400,000   293,693
jbulava          158 |         MSCHPC   106,307 |       400,000   293,693
jose               0 |         MSCHPC   106,307 |       400,000   293,693
kcleary           67 |         MSCHPC   106,307 |       400,000   293,693
lambem           514 |         MSCHPC   106,307 |       400,000   293,693
liul1            505 |         MSCHPC   106,307 |       400,000   293,693
luoq           2,782 |         MSCHPC   106,307 |       400,000   293,693
makirby        1,126 |         MSCHPC   106,307 |       400,000   293,693
mcbridne       2,142 |         MSCHPC   106,307 |       400,000   293,693
mehtav            82 |         MSCHPC   106,307 |       400,000   293,693
melbyrne         300 |         MSCHPC   106,307 |       400,000   293,693
murphd37      15,590 |         MSCHPC   106,307 |       400,000   293,693
murrayb8       2,421 |         MSCHPC   106,307 |       400,000   293,693
nobyrnes       1,513 |         MSCHPC   106,307 |       400,000   293,693
oconnm28         619 |         MSCHPC   106,307 |       400,000   293,693
ormondca       1,661 |         MSCHPC   106,307 |       400,000   293,693
osheac9          968 |         MSCHPC   106,307 |       400,000   293,693
osullm40       6,586 |         MSCHPC   106,307 |       400,000   293,693
paddy          1,319 |         MSCHPC   106,307 |       400,000   293,693
phalpin          592 |         MSCHPC   106,307 |       400,000   293,693
purdyd           173 |         MSCHPC   106,307 |       400,000   293,693
rooneydt         366 |         MSCHPC   106,307 |       400,000   293,693
sharding       1,196 |         MSCHPC   106,307 |       400,000   293,693
sikelleh         990 |         MSCHPC   106,307 |       400,000   293,693
simpsoao         509 |         MSCHPC   106,307 |       400,000   293,693
smurray4         514 |         MSCHPC   106,307 |       400,000   293,693
spellacl       1,730 |         MSCHPC   106,307 |       400,000   293,693
vetsaa            80 |         MSCHPC   106,307 |       400,000   293,693
wechen         7,593 |         MSCHPC   106,307 |       400,000   293,693
wyattg           200 |         MSCHPC   106,307 |       400,000   293,693
zhuang         8,654 |         MSCHPC   106,307 |       400,000   293,693


Acknowledgements:
=================

Note that usage of TCHPC Resources *must* be acknowledged in all publications.

Please see this page for details relevant to this cluster:

http://www.tchpc.tcd.ie/resources/acknowledgementpolicy

################################################################################
